{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-26T21:27:22.734409Z",
     "iopub.status.busy": "2025-12-26T21:27:22.734184Z",
     "iopub.status.idle": "2025-12-26T21:27:25.480055Z",
     "shell.execute_reply": "2025-12-26T21:27:25.478492Z",
     "shell.execute_reply.started": "2025-12-26T21:27:22.734387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports loaded\n"
     ]
    }
   ],
   "source": [
    "# CAFA-6 Protein Function Prediction - Baseline Model\n",
    "# This notebook trains a simple linear classifier on ESM2 embeddings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"âœ… All imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_data_path(filename):\n",
    "    \"\"\"\n",
    "    Returns the correct path whether running locally or on Kaggle.\n",
    "    \n",
    "    Args:\n",
    "        filename: Like 'Train/train_terms.tsv' or 'Test/testsuperset.fasta'\n",
    "    \n",
    "    Returns:\n",
    "        Full path to the file\n",
    "    \"\"\"\n",
    "    # Check if we're on Kaggle\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        # On Kaggle, files are in /kaggle/input/cafa-6-protein-function-prediction/\n",
    "        base_path = '/kaggle/input/cafa-6-protein-function-prediction'\n",
    "    else:\n",
    "        # Locally, files are in current directory\n",
    "        base_path = '.'\n",
    "    \n",
    "    return os.path.join(base_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading ESM2 Embeddings ===\n",
      "ðŸ“ Found ESM2 embeddings at: EMS2_Embeddings\n",
      "âœ… ESM2 Train: (142246, 1280) (142246 proteins)\n",
      "âœ… ESM2 Test: (141864, 1280) (141864 proteins)\n",
      "   Embedding dimension: 1280\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load ESM2 Embeddings\n",
    "print(\"=== Loading ESM2 Embeddings ===\")\n",
    "\n",
    "def find_embedding_dir(possible_names):\n",
    "    \"\"\"Find embedding directory (works locally and on Kaggle)\"\"\"\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for name in possible_names:\n",
    "            kaggle_path = f'/kaggle/input/{name}'\n",
    "            if os.path.exists(kaggle_path):\n",
    "                return kaggle_path\n",
    "    else:\n",
    "        for name in possible_names:\n",
    "            if os.path.exists(name):\n",
    "                return name\n",
    "    return None\n",
    "\n",
    "# Try multiple possible names (local and Kaggle)\n",
    "esm2_possible_names = [\n",
    "    'cafa-5-ems-2-embeddings-numpy',  # Kaggle dataset name\n",
    "    'EMS2_Embeddings',                 # Local name option 1\n",
    "    'ems2-embeddings'                   # Local name option 2\n",
    "]\n",
    "\n",
    "esm2_dir = find_embedding_dir(esm2_possible_names)\n",
    "\n",
    "if esm2_dir:\n",
    "    print(f\"ðŸ“ Found ESM2 embeddings at: {esm2_dir}\")\n",
    "    esm2_train_emb = np.load(f'{esm2_dir}/train_embeddings.npy')\n",
    "    esm2_train_ids = np.load(f'{esm2_dir}/train_ids.npy')\n",
    "    esm2_test_emb = np.load(f'{esm2_dir}/test_embeddings.npy')\n",
    "    esm2_test_ids = np.load(f'{esm2_dir}/test_ids.npy')\n",
    "    \n",
    "    print(f\"âœ… ESM2 Train: {esm2_train_emb.shape} ({len(esm2_train_ids)} proteins)\")\n",
    "    print(f\"âœ… ESM2 Test: {esm2_test_emb.shape} ({len(esm2_test_ids)} proteins)\")\n",
    "    print(f\"   Embedding dimension: {esm2_train_emb.shape[1]}\")\n",
    "else:\n",
    "    raise ValueError(f\"âŒ ESM2 embeddings not found! Tried: {esm2_possible_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Training Labels ===\n",
      "Training labels: 537027 protein-GO term pairs\n",
      "Unique proteins: 82404\n",
      "Unique GO terms: 26125\n",
      "\n",
      "Proteins in both: 79268\n",
      "Proteins only in labels: 3136\n",
      "Proteins only in embeddings: 62978\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load training labels and find common proteins\n",
    "print(\"=== Loading Training Labels ===\")\n",
    "\n",
    "train_terms = pd.read_csv(get_data_path('Train/train_terms.tsv'), sep='\\t')\n",
    "print(f\"Training labels: {len(train_terms)} protein-GO term pairs\")\n",
    "print(f\"Unique proteins: {train_terms['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {train_terms['term'].nunique()}\")\n",
    "\n",
    "# Find proteins that are in both embeddings and labels\n",
    "train_protein_set = set(train_terms['EntryID'].unique())\n",
    "embedding_protein_set = set(esm2_train_ids)\n",
    "overlap = train_protein_set & embedding_protein_set\n",
    "\n",
    "print(f\"\\nProteins in both: {len(overlap)}\")\n",
    "print(f\"Proteins only in labels: {len(train_protein_set - embedding_protein_set)}\")\n",
    "print(f\"Proteins only in embeddings: {len(embedding_protein_set - train_protein_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Target Matrix ===\n",
      "Using 79268 proteins that have both embeddings and labels\n",
      "Total GO terms to predict: 25980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m protein_to_idx \u001b[38;5;241m=\u001b[39m {pid: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, pid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(common_proteins)}\n\u001b[1;32m     19\u001b[0m train_terms_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_terms_filtered[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntryID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(protein_to_idx)\n\u001b[0;32m---> 21\u001b[0m target_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_terms_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprotein_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mterm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEntryID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m target_matrix \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(target_df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     30\u001b[0m all_go_terms \u001b[38;5;241m=\u001b[39m target_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:9516\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[0;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m   9499\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9500\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   9501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9512\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   9513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9514\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[0;32m-> 9516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9517\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9521\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9527\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/pivot.py:203\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m             to_unstack\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[0;32m--> 203\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43magged\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_unstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dropna:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(table\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:9935\u001b[0m, in \u001b[0;36mDataFrame.unstack\u001b[0;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[1;32m   9871\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9872\u001b[0m \u001b[38;5;124;03mPivot a level of the (necessarily hierarchical) index labels.\u001b[39;00m\n\u001b[1;32m   9873\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9931\u001b[0m \u001b[38;5;124;03mdtype: float64\u001b[39;00m\n\u001b[1;32m   9932\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9933\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[0;32m-> 9935\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   9937\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munstack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/reshape.py:504\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, DataFrame):\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m--> 504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unstack_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mstack(future_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/reshape.py:537\u001b[0m, in \u001b[0;36m_unstack_frame\u001b[0;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/reshape.py:238\u001b[0m, in \u001b[0;36m_Unstacker.get_result\u001b[0;34m(self, values, value_columns, fill_value)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust pass column labels for multi-column data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 238\u001b[0m values, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_new_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_new_columns(value_columns)\n\u001b[1;32m    240\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_index\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/reshape.py:290\u001b[0m, in \u001b[0;36m_Unstacker.get_new_values\u001b[0;34m(self, values, fill_value)\u001b[0m\n\u001b[1;32m    288\u001b[0m         dtype, fill_value \u001b[38;5;241m=\u001b[39m maybe_promote(dtype, fill_value)\n\u001b[1;32m    289\u001b[0m         new_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(result_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 290\u001b[0m         \u001b[43mnew_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m name \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    293\u001b[0m new_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(result_shape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 3: Create target matrix\n",
    "print(\"=== Creating Target Matrix ===\")\n",
    "\n",
    "common_proteins = list(overlap)\n",
    "print(f\"Using {len(common_proteins)} proteins that have both embeddings and labels\")\n",
    "\n",
    "# Filter embeddings to common proteins\n",
    "esm2_id_to_idx = {pid: idx for idx, pid in enumerate(esm2_train_ids)}\n",
    "common_indices = [esm2_id_to_idx[pid] for pid in common_proteins if pid in esm2_id_to_idx]\n",
    "esm2_train_emb_filtered = esm2_train_emb[common_indices]\n",
    "\n",
    "# Filter training labels and create target matrix\n",
    "train_terms_filtered = train_terms[train_terms['EntryID'].isin(common_proteins)].copy()\n",
    "all_go_terms = sorted(train_terms_filtered['term'].unique())\n",
    "print(f\"Total GO terms to predict: {len(all_go_terms)}\")\n",
    "\n",
    "# Create target matrix using pandas pivot\n",
    "protein_to_idx = {pid: idx for idx, pid in enumerate(common_proteins)}\n",
    "train_terms_filtered['protein_idx'] = train_terms_filtered['EntryID'].map(protein_to_idx)\n",
    "\n",
    "target_df = train_terms_filtered.pivot_table(\n",
    "    index='protein_idx',\n",
    "    columns='term',\n",
    "    values='EntryID',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "target_matrix = sp.csr_matrix(target_df.values.astype(np.float32))\n",
    "all_go_terms = target_df.columns.tolist()\n",
    "\n",
    "print(f\"âœ… Target matrix: {target_matrix.shape}\")\n",
    "print(f\"   Density: {target_matrix.nnz / (target_matrix.shape[0] * target_matrix.shape[1]):.4%}\")\n",
    "print(f\"   Avg GO terms per protein: {target_matrix.sum(axis=1).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Splitting Data ===\n",
      "Train: 63414 proteins\n",
      "Validation: 15854 proteins\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split data into train/validation\n",
    "print(\"=== Splitting Data ===\")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(common_proteins)),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = esm2_train_emb_filtered[train_idx]\n",
    "X_val = esm2_train_emb_filtered[val_idx]\n",
    "y_train = target_matrix[train_idx]\n",
    "y_val = target_matrix[val_idx]\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} proteins\")\n",
    "print(f\"Validation: {X_val.shape[0]} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Model ===\n",
      "Using device: cpu\n",
      "Model: 1280 -> 25980\n",
      "Parameters: 33,280,380\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Build PyTorch Model\n",
    "print(\"=== Building Model ===\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    \"\"\"Simple linear classifier: embedding -> GO term logits\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = ProteinClassifier(X_train.shape[1], y_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model: {X_train.shape[1]} -> {y_train.shape[1]}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating DataLoader ===\n",
      "Train batches: 1982, Val batches: 496\n",
      "Batch size: 32 (reduced for memory efficiency)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create DataLoader (Memory-optimized)\n",
    "print(\"=== Creating DataLoader ===\")\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"Dataset for protein embeddings and GO term labels (memory efficient)\"\"\"\n",
    "    def __init__(self, embeddings, targets):\n",
    "        # Keep embeddings as numpy, convert to tensor on-the-fly\n",
    "        self.embeddings = embeddings\n",
    "        # Keep targets as sparse matrix - convert to dense only for batches\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert only this one row to dense (much more memory efficient)\n",
    "        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        # Get single row from sparse matrix and convert to dense\n",
    "        target = torch.tensor(self.targets[idx].toarray().flatten(), dtype=torch.float32)\n",
    "        return emb, target\n",
    "\n",
    "train_dataset = ProteinDataset(X_train, y_train)\n",
    "val_dataset = ProteinDataset(X_val, y_val)\n",
    "\n",
    "# Reduce batch size to save memory\n",
    "batch_size = 32  # Reduced from 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "print(f\"Batch size: {batch_size} (reduced for memory efficiency)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Model ===\n",
      "Training for 3 epochs...\n",
      "  Epoch 1/3, Batch 200/1982, Loss: 0.0079\n",
      "  Epoch 1/3, Batch 400/1982, Loss: 0.0061\n",
      "  Epoch 1/3, Batch 600/1982, Loss: 0.0044\n",
      "  Epoch 1/3, Batch 800/1982, Loss: 0.0030\n",
      "  Epoch 1/3, Batch 1000/1982, Loss: 0.0027\n",
      "  Epoch 1/3, Batch 1200/1982, Loss: 0.0027\n",
      "  Epoch 1/3, Batch 1400/1982, Loss: 0.0029\n",
      "  Epoch 1/3, Batch 1600/1982, Loss: 0.0021\n",
      "  Epoch 1/3, Batch 1800/1982, Loss: 0.0026\n",
      "Epoch 1/3 - Train Loss: 0.0043, Val Loss: 0.0020\n",
      "  Epoch 2/3, Batch 200/1982, Loss: 0.0019\n",
      "  Epoch 2/3, Batch 400/1982, Loss: 0.0015\n",
      "  Epoch 2/3, Batch 600/1982, Loss: 0.0024\n",
      "  Epoch 2/3, Batch 800/1982, Loss: 0.0019\n",
      "  Epoch 2/3, Batch 1000/1982, Loss: 0.0023\n",
      "  Epoch 2/3, Batch 1200/1982, Loss: 0.0016\n",
      "  Epoch 2/3, Batch 1400/1982, Loss: 0.0014\n",
      "  Epoch 2/3, Batch 1600/1982, Loss: 0.0034\n",
      "  Epoch 2/3, Batch 1800/1982, Loss: 0.0012\n",
      "Epoch 2/3 - Train Loss: 0.0019, Val Loss: 0.0017\n",
      "  Epoch 3/3, Batch 200/1982, Loss: 0.0017\n",
      "  Epoch 3/3, Batch 400/1982, Loss: 0.0015\n",
      "  Epoch 3/3, Batch 600/1982, Loss: 0.0018\n",
      "  Epoch 3/3, Batch 800/1982, Loss: 0.0017\n",
      "  Epoch 3/3, Batch 1000/1982, Loss: 0.0017\n",
      "  Epoch 3/3, Batch 1200/1982, Loss: 0.0016\n",
      "  Epoch 3/3, Batch 1400/1982, Loss: 0.0011\n",
      "  Epoch 3/3, Batch 1600/1982, Loss: 0.0018\n",
      "  Epoch 3/3, Batch 1800/1982, Loss: 0.0018\n",
      "Epoch 3/3 - Train Loss: 0.0016, Val Loss: 0.0016\n",
      "âœ… Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training Loop (Memory-optimized)\n",
    "print(\"=== Training Model ===\")\n",
    "\n",
    "import gc  # For garbage collection\n",
    "\n",
    "num_epochs = 3\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (emb_batch, target_batch) in enumerate(train_loader):\n",
    "        emb_batch = emb_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "        outputs = model(emb_batch)\n",
    "        loss = criterion(outputs, target_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        train_loss += loss_value\n",
    "        \n",
    "        # Print progress and clean up memory\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss_value:.4f}\")\n",
    "            # Clean up memory after printing\n",
    "            del outputs, loss\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "        elif (batch_idx + 1) % 100 == 0:\n",
    "            # Clean up memory periodically (but not on print batches)\n",
    "            del outputs, loss\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for emb_batch, target_batch in val_loader:\n",
    "            emb_batch = emb_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            outputs = model(emb_batch)\n",
    "            val_loss += criterion(outputs, target_batch).item()\n",
    "            del outputs  # Free memory\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Clean up after each epoch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Making Test Predictions & Creating Submission ===\n",
      "Test embeddings: (141864, 1280)\n",
      "Test IDs: 141864\n",
      "  Processed 200/4434 batches, 3421 predictions so far\n",
      "  Processed 400/4434 batches, 6841 predictions so far\n",
      "  Processed 600/4434 batches, 10251 predictions so far\n",
      "  Processed 800/4434 batches, 13586 predictions so far\n",
      "  Processed 1000/4434 batches, 16952 predictions so far\n",
      "  Processed 1200/4434 batches, 20364 predictions so far\n",
      "  Processed 1400/4434 batches, 23706 predictions so far\n",
      "  Processed 1600/4434 batches, 27129 predictions so far\n",
      "  Processed 1800/4434 batches, 30449 predictions so far\n",
      "  Processed 2000/4434 batches, 33789 predictions so far\n",
      "  Processed 2200/4434 batches, 37130 predictions so far\n",
      "  Processed 2400/4434 batches, 40509 predictions so far\n",
      "  Processed 2600/4434 batches, 43910 predictions so far\n",
      "  Processed 2800/4434 batches, 47349 predictions so far\n",
      "  Processed 3000/4434 batches, 50755 predictions so far\n",
      "  Processed 3200/4434 batches, 54077 predictions so far\n",
      "  Processed 3400/4434 batches, 57373 predictions so far\n",
      "  Processed 3600/4434 batches, 60754 predictions so far\n",
      "  Processed 3800/4434 batches, 64106 predictions so far\n",
      "  Processed 4000/4434 batches, 67481 predictions so far\n",
      "  Processed 4200/4434 batches, 70845 predictions so far\n",
      "  Processed 4400/4434 batches, 74174 predictions so far\n",
      "\n",
      "âœ… Submission created: submission.tsv\n",
      "   Total predictions: 74734\n",
      "   File size: 2.63 MB\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Make predictions and create submission (Memory-optimized)\n",
    "print(\"=== Making Test Predictions & Creating Submission ===\")\n",
    "\n",
    "print(f\"Test embeddings: {esm2_test_emb.shape}\")\n",
    "print(f\"Test IDs: {len(esm2_test_ids)}\")\n",
    "\n",
    "threshold = 0.5\n",
    "output_file = 'submission.tsv'\n",
    "\n",
    "# Open file for writing\n",
    "with open(output_file, 'w') as f:\n",
    "    model.eval()\n",
    "    batch_size = 32\n",
    "    num_batches = (len(esm2_test_emb) + batch_size - 1) // batch_size\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(esm2_test_emb))\n",
    "            \n",
    "            # Get batch\n",
    "            batch_emb = torch.tensor(esm2_test_emb[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "            outputs = model(batch_emb)\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            \n",
    "            # Process each protein in batch and write directly to file\n",
    "            for i, protein_idx in enumerate(range(start_idx, end_idx)):\n",
    "                protein_id = esm2_test_ids[protein_idx]\n",
    "                protein_probs = probs[i]\n",
    "                \n",
    "                # Find GO terms above threshold\n",
    "                predicted_indices = np.where(protein_probs >= threshold)[0]\n",
    "                \n",
    "                # Write to file immediately\n",
    "                for go_idx in predicted_indices:\n",
    "                    go_term = all_go_terms[go_idx]\n",
    "                    confidence = float(protein_probs[go_idx])\n",
    "                    f.write(f\"{protein_id}\\t{go_term}\\t{confidence}\\n\")\n",
    "                    total_predictions += 1\n",
    "            \n",
    "            # Free memory\n",
    "            del batch_emb, outputs, probs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                print(f\"  Processed {batch_idx + 1}/{num_batches} batches, {total_predictions} predictions so far\")\n",
    "                gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… Submission created: {output_file}\")\n",
    "print(f\"   Total predictions: {total_predictions}\")\n",
    "print(f\"   File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verifying Submission ===\n",
      "Total lines in submission: 74734\n",
      "\n",
      "First 5 lines:\n",
      "Q64G17\tGO:0005515\t0.6149569749832153\n",
      "Q64G17\tGO:0005737\t0.5529754757881165\n",
      "Q75DG4\tGO:0005515\t0.7271996140480042\n",
      "Q01850\tGO:0005515\t0.7886476516723633\n",
      "P11076\tGO:0005515\t0.5794925689697266\n",
      "\n",
      "âœ… Submission file ready: submission.tsv\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Verify submission file\n",
    "print(\"=== Verifying Submission ===\")\n",
    "\n",
    "# Count lines in file\n",
    "with open(output_file, 'r') as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"Total lines in submission: {line_count}\")\n",
    "\n",
    "# Show first few lines\n",
    "print(\"\\nFirst 5 lines:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"\\nâœ… Submission file ready: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
