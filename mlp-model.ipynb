{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAFA-6 Protein Function Prediction - Multi-Aspect MLP Model\n",
        "\n",
        "**Improvements over baseline:**\n",
        "- **Multi-aspect heads**: Separate MLP models for C (Cellular Component), F (Molecular Function), P (Biological Process)\n",
        "- **MLP architecture**: Hidden layer (512 dims) with ReLU activation for non-linear learning\n",
        "- **Focal Loss**: Handles extreme class imbalance by focusing on hard examples\n",
        "- **Fmax evaluation**: Local validation matching Kaggle's evaluation\n",
        "\n",
        "**Architecture**: 3 separate MLP classifiers (1280 â†’ 512 â†’ GO terms) with Focal Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All imports loaded\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "\n",
        "print(\"âœ… All imports loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for paths (works locally and on Kaggle)\n",
        "import os\n",
        "def get_data_path(filename):\n",
        "    if os.path.exists('/kaggle/input'):\n",
        "        base_path = '/kaggle/input/cafa-6-protein-function-prediction'\n",
        "    else:\n",
        "        base_path = '.'\n",
        "    return os.path.join(base_path, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loading ESM2 Embeddings ===\n",
            "ðŸ“ Found ESM2 embeddings at: EMS2_Embeddings\n",
            "âœ… ESM2 Train: (142246, 1280) (142246 proteins)\n",
            "âœ… ESM2 Test: (141864, 1280) (141864 proteins)\n",
            "   Embedding dimension: 1280\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load ESM2 Embeddings\n",
        "print(\"=== Loading ESM2 Embeddings ===\")\n",
        "\n",
        "def find_embedding_dir(possible_names):\n",
        "    if os.path.exists('/kaggle/input'):\n",
        "        for name in possible_names:\n",
        "            kaggle_path = f'/kaggle/input/{name}'\n",
        "            if os.path.exists(kaggle_path):\n",
        "                return kaggle_path\n",
        "    else:\n",
        "        for name in possible_names:\n",
        "            if os.path.exists(name):\n",
        "                return name\n",
        "    return None\n",
        "\n",
        "esm2_possible_names = [\n",
        "    'cafa-5-ems-2-embeddings-numpy',\n",
        "    'EMS2_Embeddings',\n",
        "    'ems2-embeddings'\n",
        "]\n",
        "\n",
        "esm2_dir = find_embedding_dir(esm2_possible_names)\n",
        "\n",
        "if esm2_dir:\n",
        "    print(f\"ðŸ“ Found ESM2 embeddings at: {esm2_dir}\")\n",
        "    esm2_train_emb = np.load(f'{esm2_dir}/train_embeddings.npy')\n",
        "    esm2_train_ids = np.load(f'{esm2_dir}/train_ids.npy')\n",
        "    esm2_test_emb = np.load(f'{esm2_dir}/test_embeddings.npy')\n",
        "    esm2_test_ids = np.load(f'{esm2_dir}/test_ids.npy')\n",
        "    \n",
        "    print(f\"âœ… ESM2 Train: {esm2_train_emb.shape} ({len(esm2_train_ids)} proteins)\")\n",
        "    print(f\"âœ… ESM2 Test: {esm2_test_emb.shape} ({len(esm2_test_ids)} proteins)\")\n",
        "    print(f\"   Embedding dimension: {esm2_train_emb.shape[1]}\")\n",
        "else:\n",
        "    raise ValueError(f\"âŒ ESM2 embeddings not found! Tried: {esm2_possible_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Loading Training Labels & Splitting by Aspect ===\n",
            "Training labels: 537027 protein-GO term pairs\n",
            "Unique proteins: 82404\n",
            "Unique GO terms: 26125\n",
            "\n",
            "Proteins in both: 79268\n",
            "\n",
            "GO terms by aspect (before filtering):\n",
            "  C (Cellular Component): 2621\n",
            "  F (Molecular Function): 6578\n",
            "  P (Biological Process): 16781\n",
            "  Total: 25980\n",
            "\n",
            "=== Filtering to Allowed Terms (IA.tsv) ===\n",
            "Allowed GO terms in IA.tsv: 40122\n",
            "\n",
            "GO terms by aspect (after filtering to allowed terms):\n",
            "  C (Cellular Component): 2621\n",
            "  F (Molecular Function): 6578\n",
            "  P (Biological Process): 16781\n",
            "  Total: 25980\n",
            "\n",
            "âœ… Filtered to allowed terms - model will focus on terms that are actually scored!\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Load training labels and split by aspect (C/F/P)\n",
        "print(\"=== Loading Training Labels & Splitting by Aspect ===\")\n",
        "\n",
        "train_terms = pd.read_csv(get_data_path('Train/train_terms.tsv'), sep='\\t')\n",
        "print(f\"Training labels: {len(train_terms)} protein-GO term pairs\")\n",
        "print(f\"Unique proteins: {train_terms['EntryID'].nunique()}\")\n",
        "print(f\"Unique GO terms: {train_terms['term'].nunique()}\")\n",
        "\n",
        "# Find proteins in both embeddings and labels\n",
        "train_protein_set = set(train_terms['EntryID'].unique())\n",
        "embedding_protein_set = set(esm2_train_ids)\n",
        "common_proteins = list(train_protein_set & embedding_protein_set)\n",
        "\n",
        "print(f\"\\nProteins in both: {len(common_proteins)}\")\n",
        "\n",
        "# Filter to common proteins\n",
        "train_terms_filtered = train_terms[train_terms['EntryID'].isin(common_proteins)].copy()\n",
        "\n",
        "# Create aspect mapping\n",
        "go_term_to_aspect = dict(zip(train_terms_filtered['term'], train_terms_filtered['aspect']))\n",
        "\n",
        "# Split GO terms by aspect\n",
        "go_terms_C = sorted([term for term in train_terms_filtered['term'].unique() \n",
        "                     if go_term_to_aspect[term] == 'C'])\n",
        "go_terms_F = sorted([term for term in train_terms_filtered['term'].unique() \n",
        "                     if go_term_to_aspect[term] == 'F'])\n",
        "go_terms_P = sorted([term for term in train_terms_filtered['term'].unique() \n",
        "                     if go_term_to_aspect[term] == 'P'])\n",
        "\n",
        "print(f\"\\nGO terms by aspect (before filtering):\")\n",
        "print(f\"  C (Cellular Component): {len(go_terms_C)}\")\n",
        "print(f\"  F (Molecular Function): {len(go_terms_F)}\")\n",
        "print(f\"  P (Biological Process): {len(go_terms_P)}\")\n",
        "print(f\"  Total: {len(go_terms_C) + len(go_terms_F) + len(go_terms_P)}\")\n",
        "\n",
        "# Step 2.5: Filter to allowed terms (IA.tsv) - only terms that are actually evaluated\n",
        "print(\"\\n=== Filtering to Allowed Terms (IA.tsv) ===\")\n",
        "ia_df = pd.read_csv(get_data_path('IA.tsv'), sep='\\t', header=None, names=['term', 'ia_score'])\n",
        "allowed_terms = set(ia_df['term'].unique())\n",
        "print(f\"Allowed GO terms in IA.tsv: {len(allowed_terms)}\")\n",
        "\n",
        "# Filter each aspect to only allowed terms\n",
        "go_terms_C = [term for term in go_terms_C if term in allowed_terms]\n",
        "go_terms_F = [term for term in go_terms_F if term in allowed_terms]\n",
        "go_terms_P = [term for term in go_terms_P if term in allowed_terms]\n",
        "\n",
        "print(f\"\\nGO terms by aspect (after filtering to allowed terms):\")\n",
        "print(f\"  C (Cellular Component): {len(go_terms_C)}\")\n",
        "print(f\"  F (Molecular Function): {len(go_terms_F)}\")\n",
        "print(f\"  P (Biological Process): {len(go_terms_P)}\")\n",
        "print(f\"  Total: {len(go_terms_C) + len(go_terms_F) + len(go_terms_P)}\")\n",
        "print(f\"\\nâœ… Filtered to allowed terms - model will focus on terms that are actually scored!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating Target Matrices by Aspect ===\n",
            "  C: (79268, 2621), density: 0.0747%, avg terms/protein: 1.96\n",
            "  F: (79268, 6578), density: 0.0242%, avg terms/protein: 1.59\n",
            "  P: (79268, 16781), density: 0.0185%, avg terms/protein: 3.11\n",
            "\n",
            "âœ… Target matrices created for all aspects\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Create target matrices for each aspect\n",
        "print(\"=== Creating Target Matrices by Aspect ===\")\n",
        "\n",
        "# Filter embeddings to common proteins\n",
        "esm2_id_to_idx = {pid: idx for idx, pid in enumerate(esm2_train_ids)}\n",
        "common_indices = [esm2_id_to_idx[pid] for pid in common_proteins if pid in esm2_id_to_idx]\n",
        "esm2_train_emb_filtered = esm2_train_emb[common_indices]\n",
        "\n",
        "protein_to_idx = {pid: idx for idx, pid in enumerate(common_proteins)}\n",
        "train_terms_filtered['protein_idx'] = train_terms_filtered['EntryID'].map(protein_to_idx)\n",
        "\n",
        "# Create target matrices for each aspect\n",
        "def create_aspect_target_matrix(go_terms, aspect_name, num_proteins):\n",
        "    aspect_terms = train_terms_filtered[train_terms_filtered['term'].isin(go_terms)]\n",
        "    \n",
        "    target_df = aspect_terms.pivot_table(\n",
        "        index='protein_idx',\n",
        "        columns='term',\n",
        "        values='EntryID',\n",
        "        aggfunc='count',\n",
        "        fill_value=0\n",
        "    )\n",
        "    \n",
        "    # Ensure all proteins are present (even if they have no labels for this aspect)\n",
        "    all_protein_indices = pd.Index(range(num_proteins), name='protein_idx')\n",
        "    target_df = target_df.reindex(all_protein_indices, fill_value=0)\n",
        "    \n",
        "    # Ensure all GO terms are present (some might have no positives)\n",
        "    for term in go_terms:\n",
        "        if term not in target_df.columns:\n",
        "            target_df[term] = 0\n",
        "    \n",
        "    # Reorder columns to match go_terms order\n",
        "    target_df = target_df.reindex(columns=go_terms, fill_value=0)\n",
        "    \n",
        "    target_matrix = sp.csr_matrix(target_df.values.astype(np.float32))\n",
        "    \n",
        "    density = target_matrix.nnz / (target_matrix.shape[0] * target_matrix.shape[1])\n",
        "    print(f\"  {aspect_name}: {target_matrix.shape}, density: {density:.4%}, avg terms/protein: {target_matrix.sum(axis=1).mean():.2f}\")\n",
        "    return target_matrix\n",
        "\n",
        "num_proteins = len(common_proteins)\n",
        "y_train_C = create_aspect_target_matrix(go_terms_C, 'C', num_proteins)\n",
        "y_train_F = create_aspect_target_matrix(go_terms_F, 'F', num_proteins)\n",
        "y_train_P = create_aspect_target_matrix(go_terms_P, 'P', num_proteins)\n",
        "\n",
        "print(f\"\\nâœ… Target matrices created for all aspects\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Splitting Data ===\n",
            "Train: 63414 proteins\n",
            "Validation: 15854 proteins\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Split data into train/validation\n",
        "print(\"=== Splitting Data ===\")\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(common_proteins)),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train = esm2_train_emb_filtered[train_idx]\n",
        "X_val = esm2_train_emb_filtered[val_idx]\n",
        "\n",
        "y_train_C_split = y_train_C[train_idx]\n",
        "y_val_C_split = y_train_C[val_idx]\n",
        "\n",
        "y_train_F_split = y_train_F[train_idx]\n",
        "y_val_F_split = y_train_F[val_idx]\n",
        "\n",
        "y_train_P_split = y_train_P[train_idx]\n",
        "y_val_P_split = y_train_P[val_idx]\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]} proteins\")\n",
        "print(f\"Validation: {X_val.shape[0]} proteins\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Building Multi-Aspect Models (MLP + Focal Loss) ===\n",
            "Using device: cpu\n",
            "\n",
            "Model C: 1280 -> 512 -> 2621 GO terms\n",
            "  Parameters: 2,000,445\n",
            "Model F: 1280 -> 512 -> 6578 GO terms\n",
            "  Parameters: 4,030,386\n",
            "Model P: 1280 -> 512 -> 16781 GO terms\n",
            "  Parameters: 9,264,525\n",
            "\n",
            "âœ… Using PER-ASPECT FOCAL LOSS:\n",
            "   C: alpha=0.5, gamma=4.0, scale=10.0\n",
            "   F: alpha=0.5, gamma=4.0, scale=10.0\n",
            "   P: alpha=0.5, gamma=5.0, scale=15.0 (more aggressive for weak P aspect)\n",
            "   Tuning guide:\n",
            "   - Loss too small (<0.001) and not decreasing? â†’ Increase gamma or scale\n",
            "   - Loss too large (>0.1)? â†’ Decrease gamma or scale\n",
            "   - Loss decreasing but Fmax not improving? â†’ Try more epochs\n",
            "\n",
            "âœ… MLP models created with Focal Loss\n",
            "   Learning rate: 0.0005\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Build Multi-Aspect Models with MLP Architecture and Focal Loss\n",
        "print(\"=== Building Multi-Aspect Models (MLP + Focal Loss) ===\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class ProteinClassifier(nn.Module):\n",
        "    \"\"\"MLP classifier for one aspect (with hidden layer)\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=512):\n",
        "        super(ProteinClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for handling class imbalance\n",
        "    \n",
        "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
        "    \n",
        "    Where:\n",
        "    - p_t is predicted probability for true class\n",
        "    - alpha: weighting factor (default 0.25)\n",
        "    - gamma: focusing parameter (default 2.0, higher = more focus on hard examples)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        \n",
        "    def forward(self, logits, targets):\n",
        "        # Convert logits to probabilities\n",
        "        probs = torch.sigmoid(logits)\n",
        "        \n",
        "        # Calculate p_t (probability of true class)\n",
        "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
        "        \n",
        "        # Calculate focal weight: (1 - p_t)^gamma\n",
        "        focal_weight = (1 - p_t) ** self.gamma\n",
        "        \n",
        "        # Calculate BCE component\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
        "        \n",
        "        # Apply alpha weighting (alpha for positive class, 1-alpha for negative)\n",
        "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "        \n",
        "        # Focal loss\n",
        "        focal_loss = alpha_t * focal_weight * bce\n",
        "        \n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Create separate MLP models for each aspect\n",
        "hidden_dim = 512  # Hidden layer size\n",
        "model_C = ProteinClassifier(X_train.shape[1], len(go_terms_C), hidden_dim=hidden_dim).to(device)\n",
        "model_F = ProteinClassifier(X_train.shape[1], len(go_terms_F), hidden_dim=hidden_dim).to(device)\n",
        "model_P = ProteinClassifier(X_train.shape[1], len(go_terms_P), hidden_dim=hidden_dim).to(device)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel C: {X_train.shape[1]} -> {hidden_dim} -> {len(go_terms_C)} GO terms\")\n",
        "print(f\"  Parameters: {count_parameters(model_C):,}\")\n",
        "print(f\"Model F: {X_train.shape[1]} -> {hidden_dim} -> {len(go_terms_F)} GO terms\")\n",
        "print(f\"  Parameters: {count_parameters(model_F):,}\")\n",
        "print(f\"Model P: {X_train.shape[1]} -> {hidden_dim} -> {len(go_terms_P)} GO terms\")\n",
        "print(f\"  Parameters: {count_parameters(model_P):,}\")\n",
        "\n",
        "# Create Focal Loss for each aspect\n",
        "# Focal Loss parameters (TUNE THESE if loss is too small or not decreasing):\n",
        "# - alpha: weighting factor (0.25 = focus on positive class, try 0.5 if loss too small)\n",
        "# - gamma: focusing parameter (2.0 = moderate, try 3.0-4.0 if loss too small, 1.0 if too large)\n",
        "# - scale: loss scaling factor (1.0 = normal, try 10-100 if loss too small to provide gradients)\n",
        "# PER-ASPECT Focal Loss parameters\n",
        "# P aspect is weakest (0.10 Fmax), so we give it more aggressive settings\n",
        "# C and F aspects (already performing well ~0.34-0.40)\n",
        "focal_alpha_C = 0.5\n",
        "focal_gamma_C = 4.0\n",
        "focal_scale_C = 10.0\n",
        "\n",
        "focal_alpha_F = 0.5\n",
        "focal_gamma_F = 4.0\n",
        "focal_scale_F = 10.0\n",
        "\n",
        "# P aspect (weakest, needs more help)\n",
        "focal_alpha_P = 0.5  # Same alpha\n",
        "focal_gamma_P = 5.0   # Higher gamma = even more focus on hard examples\n",
        "focal_scale_P = 15.0  # Higher scale = stronger gradients\n",
        "\n",
        "# Wrap FocalLoss with scaling\n",
        "class ScaledFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha, gamma, scale=1.0):\n",
        "        super().__init__()\n",
        "        self.focal_loss = FocalLoss(alpha=alpha, gamma=gamma, reduction='mean')\n",
        "        self.scale = scale\n",
        "    \n",
        "    def forward(self, logits, targets):\n",
        "        return self.focal_loss(logits, targets) * self.scale\n",
        "\n",
        "criterion_C = ScaledFocalLoss(alpha=focal_alpha_C, gamma=focal_gamma_C, scale=focal_scale_C)\n",
        "criterion_F = ScaledFocalLoss(alpha=focal_alpha_F, gamma=focal_gamma_F, scale=focal_scale_F)\n",
        "criterion_P = ScaledFocalLoss(alpha=focal_alpha_P, gamma=focal_gamma_P, scale=focal_scale_P)\n",
        "\n",
        "print(f\"\\nâœ… Using PER-ASPECT FOCAL LOSS:\")\n",
        "print(f\"   C: alpha={focal_alpha_C}, gamma={focal_gamma_C}, scale={focal_scale_C}\")\n",
        "print(f\"   F: alpha={focal_alpha_F}, gamma={focal_gamma_F}, scale={focal_scale_F}\")\n",
        "print(f\"   P: alpha={focal_alpha_P}, gamma={focal_gamma_P}, scale={focal_scale_P} (more aggressive for weak P aspect)\")\n",
        "print(\"   Tuning guide:\")\n",
        "print(\"   - Loss too small (<0.001) and not decreasing? â†’ Increase gamma or scale\")\n",
        "print(\"   - Loss too large (>0.1)? â†’ Decrease gamma or scale\")\n",
        "print(\"   - Loss decreasing but Fmax not improving? â†’ Try more epochs\")\n",
        "\n",
        "# Optimizers with slightly lower learning rate (MLP can be more sensitive)\n",
        "learning_rate = 0.0005  # TUNE: try 0.001 if loss not decreasing, 0.0001 if unstable\n",
        "optimizer_C = torch.optim.Adam(model_C.parameters(), lr=learning_rate)\n",
        "optimizer_F = torch.optim.Adam(model_F.parameters(), lr=learning_rate)\n",
        "optimizer_P = torch.optim.Adam(model_P.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"\\nâœ… MLP models created with Focal Loss\")\n",
        "print(f\"   Learning rate: {learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Creating DataLoaders ===\n",
            "Train batches - C: 1982, F: 1982, P: 1982\n",
            "Val batches - C: 496, F: 496, P: 496\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Create DataLoaders for each aspect\n",
        "print(\"=== Creating DataLoaders ===\")\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, embeddings, targets):\n",
        "        self.embeddings = embeddings\n",
        "        self.targets = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
        "        target = torch.tensor(self.targets[idx].toarray().flatten(), dtype=torch.float32)\n",
        "        return emb, target\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset_C = ProteinDataset(X_train, y_train_C_split)\n",
        "train_dataset_F = ProteinDataset(X_train, y_train_F_split)\n",
        "train_dataset_P = ProteinDataset(X_train, y_train_P_split)\n",
        "\n",
        "val_dataset_C = ProteinDataset(X_val, y_val_C_split)\n",
        "val_dataset_F = ProteinDataset(X_val, y_val_F_split)\n",
        "val_dataset_P = ProteinDataset(X_val, y_val_P_split)\n",
        "\n",
        "train_loader_C = DataLoader(train_dataset_C, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "train_loader_F = DataLoader(train_dataset_F, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "train_loader_P = DataLoader(train_dataset_P, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "val_loader_C = DataLoader(val_dataset_C, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "val_loader_F = DataLoader(val_dataset_F, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "val_loader_P = DataLoader(val_dataset_P, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Train batches - C: {len(train_loader_C)}, F: {len(train_loader_F)}, P: {len(train_loader_P)}\")\n",
        "print(f\"Val batches - C: {len(val_loader_C)}, F: {len(val_loader_F)}, P: {len(val_loader_P)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Training Multi-Aspect Models ===\n",
            "\n",
            "============================================================\n",
            "TRAINING CONFIGURATION (PER-ASPECT)\n",
            "============================================================\n",
            "Epochs - C: 5, F: 5, P: 10 (P gets more!)\n",
            "Focal Loss C: alpha=0.5, gamma=4.0, scale=10.0\n",
            "Focal Loss F: alpha=0.5, gamma=4.0, scale=10.0\n",
            "Focal Loss P: alpha=0.5, gamma=5.0, scale=15.0\n",
            "Learning Rate: 0.0005\n",
            "============================================================\n",
            "\n",
            "\n",
            "Training C model...\n",
            "  Training for 5 epochs\n",
            "  Watch for: Loss should decrease, train/val should be similar\n",
            "  Initial: Loss=0.218728, Mean prob=0.5000, Max prob=0.5562\n",
            "  C Epoch 1/5, Batch 500/1982, Loss: 0.0020\n",
            "  C Epoch 1/5, Batch 1000/1982, Loss: 0.0014\n",
            "  C Epoch 1/5, Batch 1500/1982, Loss: 0.0018\n",
            "  C Epoch 1/5 - Train Loss: 0.002884, Val Loss: 0.001678\n",
            "    Sample loss: 0.218728 â†’ 0.001692, Mean prob: 0.5000 â†’ 0.0967\n",
            "  C Epoch 2/5, Batch 500/1982, Loss: 0.0016\n",
            "  C Epoch 2/5, Batch 1000/1982, Loss: 0.0018\n",
            "  C Epoch 2/5, Batch 1500/1982, Loss: 0.0015\n",
            "  C Epoch 2/5 - Train Loss: 0.001534, Val Loss: 0.001546 (Train: -46.8%, Val: -7.9%)\n",
            "    Sample loss: 0.218728 â†’ 0.001662, Mean prob: 0.5000 â†’ 0.0818\n",
            "  C Epoch 3/5, Batch 500/1982, Loss: 0.0013\n",
            "  C Epoch 3/5, Batch 1000/1982, Loss: 0.0009\n",
            "  C Epoch 3/5, Batch 1500/1982, Loss: 0.0012\n",
            "  C Epoch 3/5 - Train Loss: 0.001383, Val Loss: 0.001497 (Train: -9.9%, Val: -3.1%)\n",
            "    Sample loss: 0.218728 â†’ 0.001795, Mean prob: 0.5000 â†’ 0.0666\n",
            "  C Epoch 4/5, Batch 500/1982, Loss: 0.0018\n",
            "  C Epoch 4/5, Batch 1000/1982, Loss: 0.0010\n",
            "  C Epoch 4/5, Batch 1500/1982, Loss: 0.0015\n",
            "  C Epoch 4/5 - Train Loss: 0.001274, Val Loss: 0.001453 (Train: -7.9%, Val: -3.0%)\n",
            "    Sample loss: 0.218728 â†’ 0.001652, Mean prob: 0.5000 â†’ 0.0657\n",
            "  C Epoch 5/5, Batch 500/1982, Loss: 0.0009\n",
            "  C Epoch 5/5, Batch 1000/1982, Loss: 0.0013\n",
            "  C Epoch 5/5, Batch 1500/1982, Loss: 0.0008\n",
            "  C Epoch 5/5 - Train Loss: 0.001183, Val Loss: 0.001450 (Train: -7.2%, Val: -0.2%)\n",
            "    Sample loss: 0.218728 â†’ 0.001652, Mean prob: 0.5000 â†’ 0.0541\n",
            "  âœ… C complete! Loss decreased by -59.0% over 5 epochs\n",
            "\n",
            "Training F model...\n",
            "  Training for 5 epochs\n",
            "  Watch for: Loss should decrease, train/val should be similar\n",
            "  Initial: Loss=0.218754, Mean prob=0.5000, Max prob=0.5710\n",
            "  F Epoch 1/5, Batch 500/1982, Loss: 0.0010\n",
            "  F Epoch 1/5, Batch 1000/1982, Loss: 0.0008\n",
            "  F Epoch 1/5, Batch 1500/1982, Loss: 0.0009\n",
            "  F Epoch 1/5 - Train Loss: 0.001850, Val Loss: 0.000773\n",
            "    Sample loss: 0.218754 â†’ 0.001032, Mean prob: 0.5000 â†’ 0.0828\n",
            "  F Epoch 2/5, Batch 500/1982, Loss: 0.0008\n",
            "  F Epoch 2/5, Batch 1000/1982, Loss: 0.0005\n",
            "  F Epoch 2/5, Batch 1500/1982, Loss: 0.0004\n",
            "  F Epoch 2/5 - Train Loss: 0.000665, Val Loss: 0.000672 (Train: -64.1%, Val: -13.1%)\n",
            "    Sample loss: 0.218754 â†’ 0.000849, Mean prob: 0.5000 â†’ 0.0728\n",
            "  F Epoch 3/5, Batch 500/1982, Loss: 0.0005\n",
            "  F Epoch 3/5, Batch 1000/1982, Loss: 0.0008\n",
            "  F Epoch 3/5, Batch 1500/1982, Loss: 0.0006\n",
            "  F Epoch 3/5 - Train Loss: 0.000546, Val Loss: 0.000611 (Train: -17.8%, Val: -9.1%)\n",
            "    Sample loss: 0.218754 â†’ 0.000804, Mean prob: 0.5000 â†’ 0.0588\n",
            "  F Epoch 4/5, Batch 500/1982, Loss: 0.0007\n",
            "  F Epoch 4/5, Batch 1000/1982, Loss: 0.0006\n",
            "  F Epoch 4/5, Batch 1500/1982, Loss: 0.0005\n",
            "  F Epoch 4/5 - Train Loss: 0.000463, Val Loss: 0.000578 (Train: -15.3%, Val: -5.4%)\n",
            "    Sample loss: 0.218754 â†’ 0.000770, Mean prob: 0.5000 â†’ 0.0486\n",
            "  F Epoch 5/5, Batch 500/1982, Loss: 0.0003\n",
            "  F Epoch 5/5, Batch 1000/1982, Loss: 0.0004\n",
            "  F Epoch 5/5, Batch 1500/1982, Loss: 0.0004\n",
            "  F Epoch 5/5 - Train Loss: 0.000399, Val Loss: 0.000576 (Train: -13.8%, Val: -0.4%)\n",
            "    Sample loss: 0.218754 â†’ 0.000779, Mean prob: 0.5000 â†’ 0.0398\n",
            "  âœ… F complete! Loss decreased by -78.4% over 5 epochs\n",
            "\n",
            "Training P model...\n",
            "  Training for 10 epochs\n",
            "  Watch for: Loss should decrease, train/val should be similar\n",
            "  Initial: Loss=0.164844, Mean prob=0.5001, Max prob=0.5655\n",
            "  P Epoch 1/10, Batch 500/1982, Loss: 0.0011\n",
            "  P Epoch 1/10, Batch 1000/1982, Loss: 0.0008\n",
            "  P Epoch 1/10, Batch 1500/1982, Loss: 0.0009\n",
            "  P Epoch 1/10 - Train Loss: 0.001645, Val Loss: 0.000830\n",
            "    Sample loss: 0.164844 â†’ 0.000882, Mean prob: 0.5001 â†’ 0.1391\n",
            "  P Epoch 2/10, Batch 500/1982, Loss: 0.0008\n",
            "  P Epoch 2/10, Batch 1000/1982, Loss: 0.0005\n",
            "  P Epoch 2/10, Batch 1500/1982, Loss: 0.0007\n",
            "  P Epoch 2/10 - Train Loss: 0.000756, Val Loss: 0.000768 (Train: -54.0%, Val: -7.4%)\n",
            "    Sample loss: 0.164844 â†’ 0.000804, Mean prob: 0.5001 â†’ 0.1181\n",
            "  P Epoch 3/10, Batch 500/1982, Loss: 0.0006\n",
            "  P Epoch 3/10, Batch 1000/1982, Loss: 0.0006\n",
            "  P Epoch 3/10, Batch 1500/1982, Loss: 0.0005\n",
            "  P Epoch 3/10 - Train Loss: 0.000659, Val Loss: 0.000765 (Train: -12.9%, Val: -0.4%)\n",
            "    Sample loss: 0.164844 â†’ 0.000830, Mean prob: 0.5001 â†’ 0.0972\n",
            "  P Epoch 4/10, Batch 500/1982, Loss: 0.0008\n",
            "  P Epoch 4/10, Batch 1000/1982, Loss: 0.0006\n",
            "  P Epoch 4/10, Batch 1500/1982, Loss: 0.0004\n",
            "  P Epoch 4/10 - Train Loss: 0.000581, Val Loss: 0.000745 (Train: -11.8%, Val: -2.6%)\n",
            "    Sample loss: 0.164844 â†’ 0.000785, Mean prob: 0.5001 â†’ 0.0949\n",
            "  P Epoch 5/10, Batch 500/1982, Loss: 0.0005\n",
            "  P Epoch 5/10, Batch 1000/1982, Loss: 0.0005\n",
            "  P Epoch 5/10, Batch 1500/1982, Loss: 0.0004\n",
            "  P Epoch 5/10 - Train Loss: 0.000507, Val Loss: 0.000730 (Train: -12.8%, Val: -2.0%)\n",
            "    Sample loss: 0.164844 â†’ 0.000797, Mean prob: 0.5001 â†’ 0.0955\n",
            "  P Epoch 6/10, Batch 500/1982, Loss: 0.0004\n",
            "  P Epoch 6/10, Batch 1000/1982, Loss: 0.0005\n",
            "  P Epoch 6/10, Batch 1500/1982, Loss: 0.0004\n",
            "  P Epoch 6/10 - Train Loss: 0.000440, Val Loss: 0.000796 (Train: -13.1%, Val: +9.0%)\n",
            "    Sample loss: 0.164844 â†’ 0.000803, Mean prob: 0.5001 â†’ 0.0783\n",
            "  P Epoch 7/10, Batch 500/1982, Loss: 0.0004\n",
            "  P Epoch 7/10, Batch 1000/1982, Loss: 0.0003\n",
            "  P Epoch 7/10, Batch 1500/1982, Loss: 0.0003\n",
            "  P Epoch 7/10 - Train Loss: 0.000386, Val Loss: 0.000857 (Train: -12.2%, Val: +7.6%)\n",
            "    Sample loss: 0.164844 â†’ 0.000891, Mean prob: 0.5001 â†’ 0.0669\n",
            "  P Epoch 8/10, Batch 500/1982, Loss: 0.0003\n",
            "  P Epoch 8/10, Batch 1000/1982, Loss: 0.0002\n",
            "  P Epoch 8/10, Batch 1500/1982, Loss: 0.0004\n",
            "  P Epoch 8/10 - Train Loss: 0.000342, Val Loss: 0.000879 (Train: -11.4%, Val: +2.6%)\n",
            "    Sample loss: 0.164844 â†’ 0.000882, Mean prob: 0.5001 â†’ 0.0643\n",
            "  P Epoch 9/10, Batch 500/1982, Loss: 0.0003\n",
            "  P Epoch 9/10, Batch 1000/1982, Loss: 0.0003\n",
            "  P Epoch 9/10, Batch 1500/1982, Loss: 0.0005\n",
            "  P Epoch 9/10 - Train Loss: 0.000307, Val Loss: 0.000923 (Train: -10.3%, Val: +5.0%)\n",
            "    Sample loss: 0.164844 â†’ 0.000942, Mean prob: 0.5001 â†’ 0.0611\n",
            "  P Epoch 10/10, Batch 500/1982, Loss: 0.0003\n",
            "  P Epoch 10/10, Batch 1000/1982, Loss: 0.0004\n",
            "  P Epoch 10/10, Batch 1500/1982, Loss: 0.0002\n",
            "  P Epoch 10/10 - Train Loss: 0.000279, Val Loss: 0.000963 (Train: -9.2%, Val: +4.4%)\n",
            "    Sample loss: 0.164844 â†’ 0.000936, Mean prob: 0.5001 â†’ 0.0579\n",
            "  âœ… P complete! Loss decreased by -83.0% over 10 epochs\n",
            "\n",
            "âœ… All models trained!\n",
            "\n",
            "ðŸ’¡ Next step: Run evaluation cell to check Fmax. If Fmax is lower than expected:\n",
            "   - Loss was too small/plateaued? â†’ Increase focal_gamma (2.0â†’3.0) or focal_scale (1.0â†’10)\n",
            "   - Loss was decreasing? â†’ Try more epochs (3â†’5)\n",
            "   - Loss was unstable? â†’ Decrease learning_rate (0.0005â†’0.0001)\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Training Loop for Multi-Aspect Models\n",
        "print(\"=== Training Multi-Aspect Models ===\")\n",
        "\n",
        "def train_aspect_model(model, train_loader, val_loader, criterion, optimizer, aspect_name, num_epochs=3):\n",
        "    print(f\"\\nTraining {aspect_name} model...\")\n",
        "    print(f\"  Training for {num_epochs} epochs\")\n",
        "    print(f\"  Watch for: Loss should decrease, train/val should be similar\")\n",
        "    \n",
        "    # Get initial prediction stats (before training)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_emb, sample_target = next(iter(val_loader))\n",
        "        sample_emb = sample_emb.to(device)\n",
        "        initial_outputs = model(sample_emb)\n",
        "        initial_probs = torch.sigmoid(initial_outputs)\n",
        "        initial_mean_prob = initial_probs.mean().item()\n",
        "        initial_max_prob = initial_probs.max().item()\n",
        "        initial_loss = criterion(initial_outputs, sample_target.to(device)).item()\n",
        "    \n",
        "    print(f\"  Initial: Loss={initial_loss:.6f}, Mean prob={initial_mean_prob:.4f}, Max prob={initial_max_prob:.4f}\")\n",
        "    \n",
        "    # Track loss history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_idx, (emb_batch, target_batch) in enumerate(train_loader):\n",
        "            emb_batch = emb_batch.to(device)\n",
        "            target_batch = target_batch.to(device)\n",
        "            \n",
        "            outputs = model(emb_batch)\n",
        "            loss = criterion(outputs, target_batch)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            if (batch_idx + 1) % 500 == 0:\n",
        "                print(f\"  {aspect_name} Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "            \n",
        "            del outputs, loss\n",
        "            if (batch_idx + 1) % 200 == 0:\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "        \n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for emb_batch, target_batch in val_loader:\n",
        "                emb_batch = emb_batch.to(device)\n",
        "                target_batch = target_batch.to(device)\n",
        "                outputs = model(emb_batch)\n",
        "                val_loss += criterion(outputs, target_batch).item()\n",
        "                del outputs\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # Check if predictions are changing\n",
        "        with torch.no_grad():\n",
        "            final_outputs = model(sample_emb)\n",
        "            final_probs = torch.sigmoid(final_outputs)\n",
        "            final_mean_prob = final_probs.mean().item()\n",
        "            final_max_prob = final_probs.max().item()\n",
        "            final_loss = criterion(final_outputs, sample_target.to(device)).item()\n",
        "        \n",
        "        # Loss change indicator\n",
        "        loss_change = \"\"\n",
        "        if epoch > 0:\n",
        "            prev_train = train_losses[-2]\n",
        "            prev_val = val_losses[-2]\n",
        "            train_change = ((avg_train_loss - prev_train) / prev_train) * 100\n",
        "            val_change = ((avg_val_loss - prev_val) / prev_val) * 100\n",
        "            loss_change = f\" (Train: {train_change:+.1f}%, Val: {val_change:+.1f}%)\"\n",
        "        \n",
        "        print(f\"  {aspect_name} Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}{loss_change}\")\n",
        "        print(f\"    Sample loss: {initial_loss:.6f} â†’ {final_loss:.6f}, Mean prob: {initial_mean_prob:.4f} â†’ {final_mean_prob:.4f}\")\n",
        "        \n",
        "        # Warning if loss not decreasing\n",
        "        if epoch > 0 and avg_train_loss >= train_losses[-2] * 0.99:  # Less than 1% decrease\n",
        "            print(f\"    âš ï¸  Loss barely decreasing - consider: more epochs, higher gamma/scale, or higher LR\")\n",
        "        \n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Final summary\n",
        "    total_decrease = ((train_losses[-1] - train_losses[0]) / train_losses[0]) * 100\n",
        "    print(f\"  âœ… {aspect_name} complete! Loss decreased by {total_decrease:.1f}% over {num_epochs} epochs\")\n",
        "    if total_decrease > -5:  # Less than 5% decrease\n",
        "        print(f\"     âš ï¸  Small decrease - model may need: more epochs, better focal params, or higher LR\")\n",
        "\n",
        "# TUNE: Number of epochs PER ASPECT\n",
        "# - C and F are already good (~0.34-0.40 Fmax), so moderate epochs\n",
        "# - P is weak (0.10 Fmax), so give it MORE epochs to learn\n",
        "# - Loss still decreasing at end? â†’ Increase epochs\n",
        "# - Loss plateaued early? â†’ Don't increase epochs, tune focal loss instead\n",
        "\n",
        "num_epochs_C = 5   # C aspect (already good)\n",
        "num_epochs_F = 5   # F aspect (already good)\n",
        "num_epochs_P = 10  # P aspect (weakest, needs more training)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"TRAINING CONFIGURATION (PER-ASPECT)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Epochs - C: {num_epochs_C}, F: {num_epochs_F}, P: {num_epochs_P} (P gets more!)\")\n",
        "print(f\"Focal Loss C: alpha={focal_alpha_C}, gamma={focal_gamma_C}, scale={focal_scale_C}\")\n",
        "print(f\"Focal Loss F: alpha={focal_alpha_F}, gamma={focal_gamma_F}, scale={focal_scale_F}\")\n",
        "print(f\"Focal Loss P: alpha={focal_alpha_P}, gamma={focal_gamma_P}, scale={focal_scale_P}\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Train each aspect model with different epochs\n",
        "train_aspect_model(model_C, train_loader_C, val_loader_C, criterion_C, optimizer_C, 'C', num_epochs_C)\n",
        "train_aspect_model(model_F, train_loader_F, val_loader_F, criterion_F, optimizer_F, 'F', num_epochs_F)\n",
        "train_aspect_model(model_P, train_loader_P, val_loader_P, criterion_P, optimizer_P, 'P', num_epochs_P)\n",
        "\n",
        "print(\"\\nâœ… All models trained!\")\n",
        "print(\"\\nðŸ’¡ Next step: Run evaluation cell to check Fmax. If Fmax is lower than expected:\")\n",
        "print(\"   - Loss was too small/plateaued? â†’ Increase focal_gamma (2.0â†’3.0) or focal_scale (1.0â†’10)\")\n",
        "print(\"   - Loss was decreasing? â†’ Try more epochs (3â†’5)\")\n",
        "print(\"   - Loss was unstable? â†’ Decrease learning_rate (0.0005â†’0.0001)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluating on Validation Set (Fmax) ===\n",
            "\n",
            "Loading predictions...\n",
            "Applying top-50 filtering (C:20, F:20, P:10)...\n",
            "\n",
            "C (Cellular Component):\n",
            "  Fmax: 0.3425, Precision: 0.3206, Recall: 0.3675\n",
            "\n",
            "F (Molecular Function):\n",
            "  Fmax: 0.4026, Precision: 0.4041, Recall: 0.4010\n",
            "\n",
            "P (Biological Process):\n",
            "  Fmax: 0.1256, Precision: 0.1356, Recall: 0.1170\n",
            "\n",
            "==================================================\n",
            "Average Fmax (CAFA metric): 0.2902\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Evaluate on validation set using Fmax (per aspect) - Top-50\n",
        "print(\"=== Evaluating on Validation Set (Fmax) ===\\n\")\n",
        "\n",
        "def apply_top_k_filter(predictions, top_k):\n",
        "    \"\"\"Apply top-k filtering to predictions (keep only top-k per protein)\"\"\"\n",
        "    filtered_predictions = np.zeros_like(predictions)\n",
        "    for i in range(predictions.shape[0]):\n",
        "        top_k_indices = np.argpartition(predictions[i], -top_k)[-top_k:]\n",
        "        filtered_predictions[i, top_k_indices] = predictions[i, top_k_indices]\n",
        "    return filtered_predictions\n",
        "\n",
        "def calculate_fmax(predictions, targets, thresholds=np.linspace(0.01, 0.99, 50)):\n",
        "    \"\"\"Calculate Fmax for predictions vs targets\"\"\"\n",
        "    total_positives = np.sum(targets)\n",
        "    f1_scores = []\n",
        "    \n",
        "    for threshold in thresholds:\n",
        "        pred_binary = (predictions >= threshold).astype(bool)\n",
        "        tp = np.sum(pred_binary & targets.astype(bool))\n",
        "        fp = np.sum(pred_binary & (~targets.astype(bool)))\n",
        "        fn = total_positives - tp\n",
        "        \n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        f1_scores.append(f1)\n",
        "    \n",
        "    fmax = max(f1_scores)\n",
        "    best_threshold_idx = np.argmax(f1_scores)\n",
        "    best_threshold = thresholds[best_threshold_idx]\n",
        "    \n",
        "    # Calculate precision/recall at best threshold\n",
        "    pred_binary = (predictions >= best_threshold).astype(float)\n",
        "    tp = np.sum(pred_binary * targets)\n",
        "    fp = np.sum(pred_binary * (1 - targets))\n",
        "    fn = np.sum((1 - pred_binary) * targets)\n",
        "    \n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    \n",
        "    return fmax, precision, recall\n",
        "\n",
        "# Get predictions for each aspect\n",
        "def get_predictions(model, val_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for emb_batch, _ in val_loader:\n",
        "            emb_batch = emb_batch.to(device)\n",
        "            outputs = model(emb_batch)\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "            predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "print(\"Loading predictions...\")\n",
        "val_preds_C = get_predictions(model_C, val_loader_C, device)\n",
        "val_preds_F = get_predictions(model_F, val_loader_F, device)\n",
        "val_preds_P = get_predictions(model_P, val_loader_P, device)\n",
        "\n",
        "val_targets_C = np.vstack([target.numpy() for _, target in val_loader_C])\n",
        "val_targets_F = np.vstack([target.numpy() for _, target in val_loader_F])\n",
        "val_targets_P = np.vstack([target.numpy() for _, target in val_loader_P])\n",
        "\n",
        "# Apply top-50 filtering (C:20, F:20, P:10)\n",
        "print(\"Applying top-50 filtering (C:20, F:20, P:10)...\")\n",
        "val_preds_C = apply_top_k_filter(val_preds_C, top_k=20)\n",
        "val_preds_F = apply_top_k_filter(val_preds_F, top_k=20)\n",
        "val_preds_P = apply_top_k_filter(val_preds_P, top_k=10)\n",
        "\n",
        "# Calculate Fmax for each aspect\n",
        "fmax_C, prec_C, rec_C = calculate_fmax(val_preds_C, val_targets_C)\n",
        "fmax_F, prec_F, rec_F = calculate_fmax(val_preds_F, val_targets_F)\n",
        "fmax_P, prec_P, rec_P = calculate_fmax(val_preds_P, val_targets_P)\n",
        "\n",
        "print(f\"\\nC (Cellular Component):\")\n",
        "print(f\"  Fmax: {fmax_C:.4f}, Precision: {prec_C:.4f}, Recall: {rec_C:.4f}\")\n",
        "\n",
        "print(f\"\\nF (Molecular Function):\")\n",
        "print(f\"  Fmax: {fmax_F:.4f}, Precision: {prec_F:.4f}, Recall: {rec_F:.4f}\")\n",
        "\n",
        "print(f\"\\nP (Biological Process):\")\n",
        "print(f\"  Fmax: {fmax_P:.4f}, Precision: {prec_P:.4f}, Recall: {rec_P:.4f}\")\n",
        "\n",
        "avg_fmax = np.mean([fmax_C, fmax_F, fmax_P])\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Average Fmax (CAFA metric): {avg_fmax:.4f}\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "validation_fmax = avg_fmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Making Test Predictions & Creating Submission ===\n",
            "Test embeddings: (141864, 1280)\n",
            "Test IDs: 141864\n",
            "Using TOP-K strategy: {'C': 20, 'F': 20, 'P': 10}\n",
            "Processing in 29 chunks of 5000 proteins each...\n",
            "\n",
            "Chunk 1/29: Processing proteins 0 to 5000...\n",
            "  âœ… Chunk 1 complete!\n",
            "\n",
            "Chunk 2/29: Processing proteins 5000 to 10000...\n",
            "  âœ… Chunk 2 complete!\n",
            "\n",
            "Chunk 3/29: Processing proteins 10000 to 15000...\n",
            "  âœ… Chunk 3 complete!\n",
            "\n",
            "Chunk 4/29: Processing proteins 15000 to 20000...\n",
            "  âœ… Chunk 4 complete!\n",
            "\n",
            "Chunk 5/29: Processing proteins 20000 to 25000...\n",
            "  âœ… Chunk 5 complete!\n",
            "\n",
            "Chunk 6/29: Processing proteins 25000 to 30000...\n",
            "  âœ… Chunk 6 complete!\n",
            "\n",
            "Chunk 7/29: Processing proteins 30000 to 35000...\n",
            "  âœ… Chunk 7 complete!\n",
            "\n",
            "Chunk 8/29: Processing proteins 35000 to 40000...\n",
            "  âœ… Chunk 8 complete!\n",
            "\n",
            "Chunk 9/29: Processing proteins 40000 to 45000...\n",
            "  âœ… Chunk 9 complete!\n",
            "\n",
            "Chunk 10/29: Processing proteins 45000 to 50000...\n",
            "  âœ… Chunk 10 complete!\n",
            "\n",
            "Chunk 11/29: Processing proteins 50000 to 55000...\n",
            "  âœ… Chunk 11 complete!\n",
            "\n",
            "Chunk 12/29: Processing proteins 55000 to 60000...\n",
            "  âœ… Chunk 12 complete!\n",
            "\n",
            "Chunk 13/29: Processing proteins 60000 to 65000...\n",
            "  âœ… Chunk 13 complete!\n",
            "\n",
            "Chunk 14/29: Processing proteins 65000 to 70000...\n",
            "  âœ… Chunk 14 complete!\n",
            "\n",
            "Chunk 15/29: Processing proteins 70000 to 75000...\n",
            "  âœ… Chunk 15 complete!\n",
            "\n",
            "Chunk 16/29: Processing proteins 75000 to 80000...\n",
            "  âœ… Chunk 16 complete!\n",
            "\n",
            "Chunk 17/29: Processing proteins 80000 to 85000...\n",
            "  âœ… Chunk 17 complete!\n",
            "\n",
            "Chunk 18/29: Processing proteins 85000 to 90000...\n",
            "  âœ… Chunk 18 complete!\n",
            "\n",
            "Chunk 19/29: Processing proteins 90000 to 95000...\n",
            "  âœ… Chunk 19 complete!\n",
            "\n",
            "Chunk 20/29: Processing proteins 95000 to 100000...\n",
            "  âœ… Chunk 20 complete!\n",
            "\n",
            "Chunk 21/29: Processing proteins 100000 to 105000...\n",
            "  âœ… Chunk 21 complete!\n",
            "\n",
            "Chunk 22/29: Processing proteins 105000 to 110000...\n",
            "  âœ… Chunk 22 complete!\n",
            "\n",
            "Chunk 23/29: Processing proteins 110000 to 115000...\n",
            "  âœ… Chunk 23 complete!\n",
            "\n",
            "Chunk 24/29: Processing proteins 115000 to 120000...\n",
            "  âœ… Chunk 24 complete!\n",
            "\n",
            "Chunk 25/29: Processing proteins 120000 to 125000...\n",
            "  âœ… Chunk 25 complete!\n",
            "\n",
            "Chunk 26/29: Processing proteins 125000 to 130000...\n",
            "  âœ… Chunk 26 complete!\n",
            "\n",
            "Chunk 27/29: Processing proteins 130000 to 135000...\n",
            "  âœ… Chunk 27 complete!\n",
            "\n",
            "Chunk 28/29: Processing proteins 135000 to 140000...\n",
            "  âœ… Chunk 28 complete!\n",
            "\n",
            "Chunk 29/29: Processing proteins 140000 to 141864...\n",
            "  âœ… Chunk 29 complete!\n",
            "\n",
            "âœ… Submission created: submission.tsv\n",
            "   Total predictions: 7093200\n",
            "   File size: 253.38 MB\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Make test predictions and create submission\n",
        "print(\"=== Making Test Predictions & Creating Submission ===\")\n",
        "\n",
        "print(f\"Test embeddings: {esm2_test_emb.shape}\")\n",
        "print(f\"Test IDs: {len(esm2_test_ids)}\")\n",
        "\n",
        "output_file = 'submission.tsv'\n",
        "\n",
        "# Prediction strategy: 'top_k' or 'threshold'\n",
        "PREDICTION_STRATEGY = 'top_k'  # Change to 'threshold' for threshold-based\n",
        "\n",
        "if PREDICTION_STRATEGY == 'top_k':\n",
        "    # Top-k per aspect (best from evaluation: 50 total)\n",
        "    top_k_per_aspect = {'C': 20, 'F': 20, 'P': 10}  # Total = 50\n",
        "    print(f\"Using TOP-K strategy: {top_k_per_aspect}\")\n",
        "else:\n",
        "    # Threshold-based: output all terms above threshold\n",
        "    prediction_threshold = 0.001  # Only terms with confidence > 0.001\n",
        "    print(f\"Using THRESHOLD strategy: p > {prediction_threshold}\")\n",
        "\n",
        "# Process in chunks to avoid memory issues\n",
        "chunk_size = 5000\n",
        "num_chunks = (len(esm2_test_emb) + chunk_size - 1) // chunk_size\n",
        "\n",
        "print(f\"Processing in {num_chunks} chunks of {chunk_size} proteins each...\")\n",
        "\n",
        "total_predictions = 0\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    for chunk_idx in range(num_chunks):\n",
        "        start_idx = chunk_idx * chunk_size\n",
        "        end_idx = min((chunk_idx + 1) * chunk_size, len(esm2_test_emb))\n",
        "        chunk_proteins = end_idx - start_idx\n",
        "        \n",
        "        print(f\"\\nChunk {chunk_idx + 1}/{num_chunks}: Processing proteins {start_idx} to {end_idx}...\")\n",
        "        \n",
        "        # Get predictions for this chunk from each model\n",
        "        chunk_emb = torch.tensor(esm2_test_emb[start_idx:end_idx], dtype=torch.float32).to(device)\n",
        "        \n",
        "        model_C.eval()\n",
        "        model_F.eval()\n",
        "        model_P.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            probs_C = torch.sigmoid(model_C(chunk_emb)).cpu().numpy()\n",
        "            probs_F = torch.sigmoid(model_F(chunk_emb)).cpu().numpy()\n",
        "            probs_P = torch.sigmoid(model_P(chunk_emb)).cpu().numpy()\n",
        "        \n",
        "        # Write predictions for each protein\n",
        "        for i, protein_idx in enumerate(range(start_idx, end_idx)):\n",
        "            protein_id = esm2_test_ids[protein_idx]\n",
        "            all_predictions = []\n",
        "            \n",
        "            if PREDICTION_STRATEGY == 'top_k':\n",
        "                # Top-k per aspect\n",
        "                # C predictions\n",
        "                top_k_C = top_k_per_aspect['C']\n",
        "                top_k_indices_C = np.argpartition(probs_C[i], -top_k_C)[-top_k_C:]\n",
        "                top_k_indices_C = top_k_indices_C[np.argsort(probs_C[i][top_k_indices_C])][::-1]\n",
        "                for idx in top_k_indices_C:\n",
        "                    all_predictions.append((go_terms_C[idx], float(probs_C[i][idx])))\n",
        "                \n",
        "                # F predictions\n",
        "                top_k_F = top_k_per_aspect['F']\n",
        "                top_k_indices_F = np.argpartition(probs_F[i], -top_k_F)[-top_k_F:]\n",
        "                top_k_indices_F = top_k_indices_F[np.argsort(probs_F[i][top_k_indices_F])][::-1]\n",
        "                for idx in top_k_indices_F:\n",
        "                    all_predictions.append((go_terms_F[idx], float(probs_F[i][idx])))\n",
        "                \n",
        "                # P predictions\n",
        "                top_k_P = top_k_per_aspect['P']\n",
        "                top_k_indices_P = np.argpartition(probs_P[i], -top_k_P)[-top_k_P:]\n",
        "                top_k_indices_P = top_k_indices_P[np.argsort(probs_P[i][top_k_indices_P])][::-1]\n",
        "                for idx in top_k_indices_P:\n",
        "                    all_predictions.append((go_terms_P[idx], float(probs_P[i][idx])))\n",
        "            \n",
        "            else:\n",
        "                # Threshold-based: output all terms above threshold\n",
        "                # C predictions\n",
        "                above_threshold_C = probs_C[i] > prediction_threshold\n",
        "                for idx in np.where(above_threshold_C)[0]:\n",
        "                    all_predictions.append((go_terms_C[idx], float(probs_C[i][idx])))\n",
        "                \n",
        "                # F predictions\n",
        "                above_threshold_F = probs_F[i] > prediction_threshold\n",
        "                for idx in np.where(above_threshold_F)[0]:\n",
        "                    all_predictions.append((go_terms_F[idx], float(probs_F[i][idx])))\n",
        "                \n",
        "                # P predictions\n",
        "                above_threshold_P = probs_P[i] > prediction_threshold\n",
        "                for idx in np.where(above_threshold_P)[0]:\n",
        "                    all_predictions.append((go_terms_P[idx], float(probs_P[i][idx])))\n",
        "            \n",
        "            # Write to file (sorted by confidence descending)\n",
        "            all_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "            for go_term, confidence in all_predictions:\n",
        "                f.write(f\"{protein_id}\\t{go_term}\\t{confidence}\\n\")\n",
        "                total_predictions += 1\n",
        "        \n",
        "        # Free memory\n",
        "        del chunk_emb, probs_C, probs_F, probs_P\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        print(f\"  âœ… Chunk {chunk_idx + 1} complete!\")\n",
        "\n",
        "print(f\"\\nâœ… Submission created: {output_file}\")\n",
        "print(f\"   Total predictions: {total_predictions}\")\n",
        "print(f\"   File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
