{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-26T21:27:22.734409Z",
     "iopub.status.busy": "2025-12-26T21:27:22.734184Z",
     "iopub.status.idle": "2025-12-26T21:27:25.480055Z",
     "shell.execute_reply": "2025-12-26T21:27:25.478492Z",
     "shell.execute_reply.started": "2025-12-26T21:27:22.734387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports loaded\n"
     ]
    }
   ],
   "source": [
    "# CAFA-6 Protein Function Prediction - Baseline Model\n",
    "# This notebook trains a simple linear classifier on ESM2 embeddings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"âœ… All imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_data_path(filename):\n",
    "    \"\"\"\n",
    "    Returns the correct path whether running locally or on Kaggle.\n",
    "    \n",
    "    Args:\n",
    "        filename: Like 'Train/train_terms.tsv' or 'Test/testsuperset.fasta'\n",
    "    \n",
    "    Returns:\n",
    "        Full path to the file\n",
    "    \"\"\"\n",
    "    # Check if we're on Kaggle\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        # On Kaggle, files are in /kaggle/input/cafa-6-protein-function-prediction/\n",
    "        base_path = '/kaggle/input/cafa-6-protein-function-prediction'\n",
    "    else:\n",
    "        # Locally, files are in current directory\n",
    "        base_path = '.'\n",
    "    \n",
    "    return os.path.join(base_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading ESM2 Embeddings ===\n",
      "ðŸ“ Found ESM2 embeddings at: EMS2_Embeddings\n",
      "âœ… ESM2 Train: (142246, 1280) (142246 proteins)\n",
      "âœ… ESM2 Test: (141864, 1280) (141864 proteins)\n",
      "   Embedding dimension: 1280\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load ESM2 Embeddings\n",
    "print(\"=== Loading ESM2 Embeddings ===\")\n",
    "\n",
    "def find_embedding_dir(possible_names):\n",
    "    \"\"\"Find embedding directory (works locally and on Kaggle)\"\"\"\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        for name in possible_names:\n",
    "            kaggle_path = f'/kaggle/input/{name}'\n",
    "            if os.path.exists(kaggle_path):\n",
    "                return kaggle_path\n",
    "    else:\n",
    "        for name in possible_names:\n",
    "            if os.path.exists(name):\n",
    "                return name\n",
    "    return None\n",
    "\n",
    "# Try multiple possible names (local and Kaggle)\n",
    "esm2_possible_names = [\n",
    "    'cafa-5-ems-2-embeddings-numpy',  # Kaggle dataset name\n",
    "    'EMS2_Embeddings',                 # Local name option 1\n",
    "    'ems2-embeddings'                   # Local name option 2\n",
    "]\n",
    "\n",
    "esm2_dir = find_embedding_dir(esm2_possible_names)\n",
    "\n",
    "if esm2_dir:\n",
    "    print(f\"ðŸ“ Found ESM2 embeddings at: {esm2_dir}\")\n",
    "    esm2_train_emb = np.load(f'{esm2_dir}/train_embeddings.npy')\n",
    "    esm2_train_ids = np.load(f'{esm2_dir}/train_ids.npy')\n",
    "    esm2_test_emb = np.load(f'{esm2_dir}/test_embeddings.npy')\n",
    "    esm2_test_ids = np.load(f'{esm2_dir}/test_ids.npy')\n",
    "    \n",
    "    print(f\"âœ… ESM2 Train: {esm2_train_emb.shape} ({len(esm2_train_ids)} proteins)\")\n",
    "    print(f\"âœ… ESM2 Test: {esm2_test_emb.shape} ({len(esm2_test_ids)} proteins)\")\n",
    "    print(f\"   Embedding dimension: {esm2_train_emb.shape[1]}\")\n",
    "else:\n",
    "    raise ValueError(f\"âŒ ESM2 embeddings not found! Tried: {esm2_possible_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Training Labels ===\n",
      "Training labels: 537027 protein-GO term pairs\n",
      "Unique proteins: 82404\n",
      "Unique GO terms: 26125\n",
      "\n",
      "Proteins in both: 79268\n",
      "Proteins only in labels: 3136\n",
      "Proteins only in embeddings: 62978\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load training labels and find common proteins\n",
    "print(\"=== Loading Training Labels ===\")\n",
    "\n",
    "train_terms = pd.read_csv(get_data_path('Train/train_terms.tsv'), sep='\\t')\n",
    "print(f\"Training labels: {len(train_terms)} protein-GO term pairs\")\n",
    "print(f\"Unique proteins: {train_terms['EntryID'].nunique()}\")\n",
    "print(f\"Unique GO terms: {train_terms['term'].nunique()}\")\n",
    "\n",
    "# Find proteins that are in both embeddings and labels\n",
    "train_protein_set = set(train_terms['EntryID'].unique())\n",
    "embedding_protein_set = set(esm2_train_ids)\n",
    "overlap = train_protein_set & embedding_protein_set\n",
    "\n",
    "print(f\"\\nProteins in both: {len(overlap)}\")\n",
    "print(f\"Proteins only in labels: {len(train_protein_set - embedding_protein_set)}\")\n",
    "print(f\"Proteins only in embeddings: {len(embedding_protein_set - train_protein_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Target Matrix ===\n",
      "Using 79268 proteins that have both embeddings and labels\n",
      "Total GO terms to predict: 25980\n",
      "âœ… Target matrix: (79268, 25980)\n",
      "   Density: 0.0256%\n",
      "   Avg GO terms per protein: 6.66\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create target matrix\n",
    "print(\"=== Creating Target Matrix ===\")\n",
    "\n",
    "common_proteins = list(overlap)\n",
    "print(f\"Using {len(common_proteins)} proteins that have both embeddings and labels\")\n",
    "\n",
    "# Filter embeddings to common proteins\n",
    "esm2_id_to_idx = {pid: idx for idx, pid in enumerate(esm2_train_ids)}\n",
    "common_indices = [esm2_id_to_idx[pid] for pid in common_proteins if pid in esm2_id_to_idx]\n",
    "esm2_train_emb_filtered = esm2_train_emb[common_indices]\n",
    "\n",
    "# Filter training labels and create target matrix\n",
    "train_terms_filtered = train_terms[train_terms['EntryID'].isin(common_proteins)].copy()\n",
    "all_go_terms = sorted(train_terms_filtered['term'].unique())\n",
    "print(f\"Total GO terms to predict: {len(all_go_terms)}\")\n",
    "\n",
    "# Create target matrix using pandas pivot\n",
    "protein_to_idx = {pid: idx for idx, pid in enumerate(common_proteins)}\n",
    "train_terms_filtered['protein_idx'] = train_terms_filtered['EntryID'].map(protein_to_idx)\n",
    "\n",
    "target_df = train_terms_filtered.pivot_table(\n",
    "    index='protein_idx',\n",
    "    columns='term',\n",
    "    values='EntryID',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "target_matrix = sp.csr_matrix(target_df.values.astype(np.float32))\n",
    "all_go_terms = target_df.columns.tolist()\n",
    "\n",
    "print(f\"âœ… Target matrix: {target_matrix.shape}\")\n",
    "print(f\"   Density: {target_matrix.nnz / (target_matrix.shape[0] * target_matrix.shape[1]):.4%}\")\n",
    "print(f\"   Avg GO terms per protein: {target_matrix.sum(axis=1).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Splitting Data ===\n",
      "Train: 63414 proteins\n",
      "Validation: 15854 proteins\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split data into train/validation\n",
    "print(\"=== Splitting Data ===\")\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(common_proteins)),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = esm2_train_emb_filtered[train_idx]\n",
    "X_val = esm2_train_emb_filtered[val_idx]\n",
    "y_train = target_matrix[train_idx]\n",
    "y_val = target_matrix[val_idx]\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} proteins\")\n",
    "print(f\"Validation: {X_val.shape[0]} proteins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Model ===\n",
      "Using device: cpu\n",
      "Model: 1280 -> 25980\n",
      "Parameters: 33,280,380\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Build PyTorch Model\n",
    "print(\"=== Building Model ===\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class ProteinClassifier(nn.Module):\n",
    "    \"\"\"Simple linear classifier: embedding -> GO term logits\"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = ProteinClassifier(X_train.shape[1], y_train.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Model: {X_train.shape[1]} -> {y_train.shape[1]}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating DataLoader ===\n",
      "Train batches: 1982, Val batches: 496\n",
      "Batch size: 32 (reduced for memory efficiency)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create DataLoader (Memory-optimized)\n",
    "print(\"=== Creating DataLoader ===\")\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"Dataset for protein embeddings and GO term labels (memory efficient)\"\"\"\n",
    "    def __init__(self, embeddings, targets):\n",
    "        # Keep embeddings as numpy, convert to tensor on-the-fly\n",
    "        self.embeddings = embeddings\n",
    "        # Keep targets as sparse matrix - convert to dense only for batches\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert only this one row to dense (much more memory efficient)\n",
    "        emb = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        # Get single row from sparse matrix and convert to dense\n",
    "        target = torch.tensor(self.targets[idx].toarray().flatten(), dtype=torch.float32)\n",
    "        return emb, target\n",
    "\n",
    "train_dataset = ProteinDataset(X_train, y_train)\n",
    "val_dataset = ProteinDataset(X_val, y_val)\n",
    "\n",
    "# Reduce batch size to save memory\n",
    "batch_size = 32  # Reduced from 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "print(f\"Batch size: {batch_size} (reduced for memory efficiency)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Model ===\n",
      "Training for 3 epochs...\n",
      "  Epoch 1/3, Batch 200/1982, Loss: 0.0164\n",
      "  Epoch 1/3, Batch 400/1982, Loss: 0.0092\n",
      "  Epoch 1/3, Batch 600/1982, Loss: 0.0048\n",
      "  Epoch 1/3, Batch 800/1982, Loss: 0.0033\n",
      "  Epoch 1/3, Batch 1000/1982, Loss: 0.0036\n",
      "  Epoch 1/3, Batch 1200/1982, Loss: 0.0030\n",
      "  Epoch 1/3, Batch 1400/1982, Loss: 0.0032\n",
      "  Epoch 1/3, Batch 1600/1982, Loss: 0.0031\n",
      "  Epoch 1/3, Batch 1800/1982, Loss: 0.0027\n",
      "Epoch 1/3 - Train Loss: 0.0151, Val Loss: 0.0022\n",
      "  Epoch 2/3, Batch 200/1982, Loss: 0.0018\n",
      "  Epoch 2/3, Batch 400/1982, Loss: 0.0026\n",
      "  Epoch 2/3, Batch 600/1982, Loss: 0.0018\n",
      "  Epoch 2/3, Batch 800/1982, Loss: 0.0018\n",
      "  Epoch 2/3, Batch 1000/1982, Loss: 0.0015\n",
      "  Epoch 2/3, Batch 1200/1982, Loss: 0.0015\n",
      "  Epoch 2/3, Batch 1400/1982, Loss: 0.0020\n",
      "  Epoch 2/3, Batch 1600/1982, Loss: 0.0015\n",
      "  Epoch 2/3, Batch 1800/1982, Loss: 0.0017\n",
      "Epoch 2/3 - Train Loss: 0.0019, Val Loss: 0.0018\n",
      "  Epoch 3/3, Batch 200/1982, Loss: 0.0014\n",
      "  Epoch 3/3, Batch 400/1982, Loss: 0.0016\n",
      "  Epoch 3/3, Batch 600/1982, Loss: 0.0018\n",
      "  Epoch 3/3, Batch 800/1982, Loss: 0.0026\n",
      "  Epoch 3/3, Batch 1000/1982, Loss: 0.0014\n",
      "  Epoch 3/3, Batch 1200/1982, Loss: 0.0012\n",
      "  Epoch 3/3, Batch 1400/1982, Loss: 0.0014\n",
      "  Epoch 3/3, Batch 1600/1982, Loss: 0.0015\n",
      "  Epoch 3/3, Batch 1800/1982, Loss: 0.0021\n",
      "Epoch 3/3 - Train Loss: 0.0016, Val Loss: 0.0016\n",
      "âœ… Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training Loop (Memory-optimized)\n",
    "print(\"=== Training Model ===\")\n",
    "\n",
    "import gc  # For garbage collection\n",
    "\n",
    "num_epochs = 3\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (emb_batch, target_batch) in enumerate(train_loader):\n",
    "        emb_batch = emb_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "        outputs = model(emb_batch)\n",
    "        loss = criterion(outputs, target_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = loss.item()\n",
    "        train_loss += loss_value\n",
    "        \n",
    "        # Print progress and clean up memory\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss_value:.4f}\")\n",
    "            # Clean up memory after printing\n",
    "            del outputs, loss\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "        elif (batch_idx + 1) % 100 == 0:\n",
    "            # Clean up memory periodically (but not on print batches)\n",
    "            del outputs, loss\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for emb_batch, target_batch in val_loader:\n",
    "            emb_batch = emb_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            outputs = model(emb_batch)\n",
    "            val_loss += criterion(outputs, target_batch).item()\n",
    "            del outputs  # Free memory\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Clean up after each epoch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Making Test Predictions & Creating Submission ===\n",
      "Test embeddings: (141864, 1280)\n",
      "Test IDs: 141864\n",
      "Starting prediction loop: 4434 batches total\n",
      "\n",
      "âœ… Submission created: submission.tsv\n",
      "   Total predictions: 7093200\n",
      "   File size: 260.77 MB\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Make predictions and create submission (Memory-optimized)\n",
    "print(\"=== Making Test Predictions & Creating Submission ===\")\n",
    "\n",
    "print(f\"Test embeddings: {esm2_test_emb.shape}\")\n",
    "print(f\"Test IDs: {len(esm2_test_ids)}\")\n",
    "\n",
    "top_k = 50  # Predict top 50 GO terms per protein (like baseline)\n",
    "output_file = 'submission.tsv'\n",
    "\n",
    "# Open file for writing\n",
    "with open(output_file, 'w') as f:\n",
    "        model.eval()\n",
    "        batch_size = 32\n",
    "        num_batches = (len(esm2_test_emb) + batch_size - 1) // batch_size\n",
    "        total_predictions = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(f\"Starting prediction loop: {num_batches} batches total\")\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min((batch_idx + 1) * batch_size, len(esm2_test_emb))\n",
    "                \n",
    "                # Get batch\n",
    "                batch_emb = torch.tensor(esm2_test_emb[start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "                outputs = model(batch_emb)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                \n",
    "                # Process each protein in batch and write directly to file\n",
    "                for i, protein_idx in enumerate(range(start_idx, end_idx)):\n",
    "                    protein_id = esm2_test_ids[protein_idx]\n",
    "                    protein_probs = probs[i]\n",
    "                    \n",
    "                    # Get top-k GO terms (highest probabilities) - using argpartition for speed\n",
    "                    top_k_indices = np.argpartition(protein_probs, -top_k)[-top_k:]\n",
    "                    # Sort only the top-k (much faster than sorting all 25k)\n",
    "                    top_k_indices = top_k_indices[np.argsort(protein_probs[top_k_indices])][::-1]\n",
    "                    \n",
    "                    # Write to file immediately\n",
    "                    for go_idx in top_k_indices:\n",
    "                        go_term = all_go_terms[go_idx]\n",
    "                        confidence = float(protein_probs[go_idx])\n",
    "                        f.write(f\"{protein_id}\\t{go_term}\\t{confidence}\\n\")\n",
    "                        total_predictions += 1\n",
    "            \n",
    "            # Free memory\n",
    "            del batch_emb, outputs, probs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Print progress more frequently\n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Processed {batch_idx + 1}/{num_batches} batches, {total_predictions} predictions so far\")\n",
    "            if (batch_idx + 1) % 200 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… Submission created: {output_file}\")\n",
    "print(f\"   Total predictions: {total_predictions}\")\n",
    "print(f\"   File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verifying Submission ===\n",
      "Total lines in submission: 7093200\n",
      "\n",
      "First 5 lines:\n",
      "Q9ZSA8\tGO:0005515\t0.0990348532795906\n",
      "Q9ZSA8\tGO:0005634\t0.08254110813140869\n",
      "Q9ZSA8\tGO:0005829\t0.06589846312999725\n",
      "Q9ZSA8\tGO:0005739\t0.04138588160276413\n",
      "Q9ZSA8\tGO:0005886\t0.0381326787173748\n",
      "\n",
      "âœ… Submission file ready: submission.tsv\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Verify submission file\n",
    "print(\"=== Verifying Submission ===\")\n",
    "\n",
    "# Count lines in file\n",
    "with open(output_file, 'r') as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"Total lines in submission: {line_count}\")\n",
    "\n",
    "# Show first few lines\n",
    "print(\"\\nFirst 5 lines:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:\n",
    "            print(line.strip())\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"\\nâœ… Submission file ready: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14875579,
     "sourceId": 116062,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
